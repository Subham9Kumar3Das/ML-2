{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd2dd63-7f7c-43df-9841-132c304adb04",
   "metadata": {},
   "source": [
    "* Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?\n",
    "* Q2: How can we reduce overfitting? Explain in brief.\n",
    "* Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "* Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "* Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "* Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "* Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60875aba-f9a9-41ca-b5ee-69c6ba40c8f2",
   "metadata": {},
   "source": [
    "* Q1:\n",
    "\n",
    "Overfitting: Occurs when a model learns the training data too well, capturing noise and specific patterns that don't generalize well to new, unseen data. Consequences include poor performance on new data and sensitivity to noise in the training set.\n",
    "\n",
    "Underfitting: Happens when a model is too simple to capture the underlying patterns in the data. Consequences include low accuracy even on the training data and an inability to generalize to new data.\n",
    "\n",
    "To mitigate overfitting and underfitting, techniques such as cross-validation, proper dataset splitting, feature engineering, and model regularization can be employed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91739c1-200d-4497-878c-2edbc2622fdd",
   "metadata": {},
   "source": [
    "* Q2:\n",
    "\n",
    "To reduce overfitting:\n",
    "\n",
    "Use more data to train the model.\n",
    "\n",
    "Simplify the model architecture.\n",
    "\n",
    "Apply regularization techniques (e.g., L1/L2 regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f170a9-eed3-4625-a272-438cb81aff3c",
   "metadata": {},
   "source": [
    "* Q3:\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the data. Scenarios for underfitting include:\n",
    "\n",
    "Using a linear model for a non-linear problem.\n",
    "\n",
    "Insufficiently complex model for the dataset.\n",
    "\n",
    "Not training the model long enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c8c9e-1474-4858-94fb-683a0fc1a5f6",
   "metadata": {},
   "source": [
    "* Q4:\n",
    "\n",
    "Bias-variance tradeoff refers to the balance between bias (error from overly simplistic assumptions) and variance (sensitivity to small fluctuations in the training data). High bias can lead to underfitting, while high variance can lead to overfitting. Achieving a good tradeoff results in a model that generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e077e-0697-4359-b9b4-6543fb10a9da",
   "metadata": {},
   "source": [
    "* Q5:\n",
    "    \n",
    "Common methods for detecting overfitting and underfitting:\n",
    "\n",
    "Cross-validation: Assessing model performance on different subsets of the data.\n",
    "\n",
    "Learning curves: Plotting training and validation performance against the number of training examples or epochs.\n",
    "\n",
    "Model evaluation metrics: Monitoring metrics like accuracy, precision, recall, and F1 score on training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e35989-7a19-4354-b166-b01fed128102",
   "metadata": {},
   "source": [
    "* Q6:\n",
    "\n",
    "Bias represents the error introduced by approximating a real-world problem with a too-simplistic model, while variance is the model's sensitivity to fluctuations in the training data. High bias models include linear regression on a non-linear problem, and high variance models may be highly complex models fitting noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b7ec6-b15c-4477-9715-e686678f23cd",
   "metadata": {},
   "source": [
    "* Q7.\n",
    "\n",
    "Regularization in machine learning is a technique to prevent overfitting, where a model learns the training data too well but struggles to generalize to new, unseen data. Overfitting occurs when a model becomes too complex and captures noise in the training data instead of the underlying patterns.\n",
    "\n",
    "Common regularization techniques and how they work:\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "How it works: Adds the absolute values of the coefficients to the loss function.\n",
    "Effect: Encourages the model to use fewer features, effectively selecting a subset of features and pushing others towards zero.\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "How it works: Adds the squared values of the coefficients to the loss function.\n",
    "Effect: Penalizes large coefficients, preventing any single feature from dominating the model and promoting a more balanced use of all features.\n",
    "\n",
    "Dropout:\n",
    "\n",
    "How it works: Randomly deactivates a fraction of neurons during training, forcing the model to learn redundant representations.\n",
    "Effect: Prevents the model from relying too much on specific features, improving generalization.\n",
    "\n",
    "Early Stopping:\n",
    "\n",
    "How it works: Halts training when the model's performance on a validation set starts to degrade.\n",
    "Effect: Prevents the model from learning noise in the training data by stopping before it becomes too specialized.\n",
    "Regularization methods aim to strike a balance between fitting the training data well and avoiding overemphasis on specific features or patterns that might not generalize to new data. They help create models that generalize better to unseen examples, making them more robust and applicable in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17c8e4-2ef9-4ece-8b86-d693f4a6f6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
